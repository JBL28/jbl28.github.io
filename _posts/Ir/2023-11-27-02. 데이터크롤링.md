---
title:  "(IR실습) 데이터크롤러 구현하기" 
excerpt: "(IR실습) 데이터크롤러 구현하기"

categories:
  - Ir
tags:
  - [IR, 정보검색, 데이터크롤러, ]

toc: true
toc_sticky: true
 
date: 2023-11-27 15:00:00
last_modified_at: 2023-11-27

---
이번에는 셀레니움을 이용해 데이터 크롤러를 구현해볼 것 입니다.<br>

깊이가 깊지 않은 간단한 구조의 페이지를 크롤링 하는 크롤러를 만들 것 입니다.<br>

다음은 크롤러를 구현한 파이썬 코드입니다.<br>
```python
# -*- coding: utf-8 -*-
"""
Created on Wed Sep 13 19:42:55 2023

@author: 정복
"""
import math
import time

from selenium import webdriver  # 셀레니움을 활성화
from selenium.webdriver.common.by import By

dr = webdriver.Chrome()  # 크롬 드라이버를 실행하는 명령어를 dr로 지정
dr2 = webdriver.Chrome()    #하위 문서를 열 드라이버
driver = dr.get('https://terms.naver.com/list.naver?cid=60476&categoryId=60476')  # 드라이버를 통해 url의 웹 페이지를 오픈

selector = "#content > div.contents_list_wrap.sub > ul"         #이 코드부터
links = dr.find_elements(By.CSS_SELECTOR, selector)

start = time.time()
crawlCounter = 0;

for i in links:
    contents = i.find_elements(By.CLASS_NAME, "contents_sub.active")
    for j in contents:
        contents_item = j.find_elements(By.TAG_NAME, "ul")
        for a in contents_item:
            sub_item = a.find_element(By.TAG_NAME, "li")
            item = sub_item.find_element(By.TAG_NAME, "a")      #여기까지는 링크가 담긴 html 구조를 찾는 부분
            href = item.get_attribute("href")                   #해당 코드에서 url을 따온다.
            
            driver2 = dr2.get(href)                             #해당 URL의 페이지를 새로운 드라이버에 띄우고
            title = dr2.find_element(By.CLASS_NAME, "headword").text #제목을 따온다
            print(title, "문서 크롤링 중..")                          #제목을 이용해 안내 메시지 출력
            texts = dr2.find_element(By.CLASS_NAME, "size_ct_v2").text #내용도 따온다
            texts = texts.replace("\n", " ")
            
            f = open(title+".txt", "w+", encoding = "utf-8")        #제목을 텍스트파일의 제목으로 설정하고,
            f.write(title + "\n\n\n" + texts)                       #크롤링한 제목과 내용을 저장한다.
            f.close()
            crawlCounter = crawlCounter + 1
            
end = time.time()

print(f"{end - start:.5f} 초 동안" + str(crawlCounter) + "개의 문서를 크롤링했습니다.")
```
<br><br>
해당 코드는 두개의 웹드라이버를 사용하는 코드입니다.<br>
셀레니움과 데이터크롤러, 웹드라이버에 대한 내용은 추후 더 자세하게 작성하겠습니다.<br>
for문 전까지는 시작 URL 설정과 셀레니움 설정에 대한 부분입니다.<br>
우리가 자세하게 봐야할 부분은 그 다음 부분이죠.<br><br>
<br><br>
![네이버악기백과](/assets/images/Ir/02/네이버악기백과_.png "네이버악기백과")<br>
a변수를 사용하는 for문에서, 페이지의 HTML 주소를 분석해 소스 URL이 있는 위치를 알아냅니다.<br>
find_elements() 함수를 사용하는 부분이 그 부분입니다.<br>
그 다음, 우리는 소스 URL의 본문에 있는 URL들을 가져와 두번째 웹브라우저에서 페이지를 긁어 txt파일로 저장하게 됩니다.<br>
더이상 크롤링 할 URL이 없을 때까지 위 과정을 반복합니다.<br><br><br>

실행화면을 보여드리겠습니다.
>실행과정<br>
>![실행과정](/assets/images/Ir/02/실행과정_.png "실행과정")<br><br><br>


>저장된파일<br>
>![저장된파일](/assets/images/Ir/02/저장된파일_.png "저장된파일")<br><br>

정상적으로 텍스트파일로 저장했습니다!<br>
다음은 형태소해석과 불용어처리를 구현하겠습니다.<br><br>



읽어주셔서 감사합니다! <br>틀린 부분이 있으면 언제든 댓글 달아주세요!
{: .notice--primary} 
